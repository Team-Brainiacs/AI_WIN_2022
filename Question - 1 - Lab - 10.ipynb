{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6549cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At r(S) : -0.04\n",
      "Total Iterations :267\n",
      "[['U', 'U', 'L', 'L'], ['L', 'L', 'L', 'L'], ['U', 'U', 'U', 'L']]\n",
      "--- --- --- --- --- --- ---\n",
      "-1.23|-0.83|-0.28| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      "-1.47| 0.00|-0.87| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      "-1.55|-1.47|-1.22|-1.17|\n",
      "\n",
      "************************************\n",
      "\n",
      "At r(S) : -2\n",
      "Total Iterations :339\n",
      "[['U', 'U', 'U', 'L'], ['L', 'U', 'U', 'L'], ['U', 'U', 'U', 'L']]\n",
      "--- --- --- --- --- --- ---\n",
      "-59.71|-46.01|-24.32| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      "-65.41| 0.00|-21.94| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      "-63.10|-52.80|-34.49|-20.75|\n",
      "\n",
      "************************************\n",
      "\n",
      "At r(S) : 0.1\n",
      "Total Iterations :279\n",
      "[['L', 'D', 'D', 'D'], ['U', 'D', 'L', 'L'], ['R', 'D', 'D', 'D']]\n",
      "--- --- --- --- --- --- ---\n",
      " 2.95| 2.39| 1.44| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      " 3.10| 0.00| 0.63| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      " 2.85| 2.20| 1.15| 0.23|\n",
      "\n",
      "************************************\n",
      "\n",
      "At r(S) : 0.02\n",
      "Total Iterations :239\n",
      "[['L', 'D', 'D', 'D'], ['L', 'L', 'L', 'L'], ['L', 'D', 'U', 'L']]\n",
      "--- --- --- --- --- --- ---\n",
      " 0.56| 0.55| 0.46| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      " 0.49| 0.00|-0.23| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      " 0.34| 0.11|-0.20|-0.57|\n",
      "\n",
      "************************************\n",
      "\n",
      "At r(S) : 1\n",
      "Total Iterations :326\n",
      "[['L', 'D', 'D', 'D'], ['U', 'D', 'R', 'D'], ['R', 'D', 'D', 'D']]\n",
      "--- --- --- --- --- --- ---\n",
      " 29.80| 23.14| 12.48| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      " 32.46| 0.00| 10.30| 0.00|\n",
      "--- --- --- --- --- --- ---\n",
      " 31.11| 25.77| 16.43| 9.22|\n",
      "\n",
      "************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "blocked_State = [(1,1)]\n",
    "\n",
    "possible_actions = ['L','R','U','D']\n",
    "terminate_states = ((1,3),(2,3))\n",
    "\n",
    "\n",
    "# non - deterministic action\n",
    "prob_actions = {'L':0.25,'R':0.25,'U':0.25,'D':0.25}\n",
    "\n",
    "# environment action corresponding to Agent\n",
    "environment_left = {'L':'D','R':'U','U':'L','D':'R'}\n",
    "environment_right = {'L':'U','R':'D','U':'R','D':'L'}\n",
    "\n",
    "def is_valid(i,j):\n",
    "    return (i,j) not in blocked_State and i >= 0 and i < 3 and j >= 0 and j < 4\n",
    "\n",
    "def print_values(V):\n",
    "  for i in range(2,-1,-1):\n",
    "    print(\"--- --- --- --- --- --- ---\")\n",
    "    for j in range(4):\n",
    "      v = V[i][j]\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "def func(action,i,j):\n",
    "    if action == 'L':\n",
    "        new_state = (i,j-1)\n",
    "    elif action == 'R':\n",
    "        new_state = (i,j+1)\n",
    "    elif action == 'U':\n",
    "        new_state = (i+1,j)\n",
    "    else:\n",
    "        new_state = (i-1,j)   \n",
    "\n",
    "    return new_state\n",
    "\n",
    "def find_value_function(i,j,reward,reward_matrix,tolerance_rate=1):\n",
    "    value = 0\n",
    "    for action in possible_actions:\n",
    "        # desired action with 0.8 probability\n",
    "        state_x,state_y = func(action,i,j)\n",
    "        if is_valid(state_x,state_y):\n",
    "            desired_action_value = (reward_matrix[state_x][state_y] + tolerance_rate*V_pie[state_x][state_y])\n",
    "        else:\n",
    "            desired_action_value = (reward_matrix[i][j] + tolerance_rate*V_pie[i][j])\n",
    "        \n",
    "        # environment action with 0.1 probability\n",
    "        state_x,state_y = func(environment_left[action],i,j)\n",
    "        if is_valid(state_x,state_y):\n",
    "            env_action_left_value = (reward_matrix[state_x][state_y] + tolerance_rate*V_pie[state_x][state_y])\n",
    "        else:\n",
    "            env_action_left_value = (reward_matrix[i][j] + tolerance_rate*V_pie[i][j])\n",
    "        \n",
    "        # environment action with 0.1 probability \n",
    "        state_x,state_y = func(environment_right[action],i,j)\n",
    "        if is_valid(state_x,state_y):\n",
    "            env_action_right_value = (reward_matrix[state_x][state_y] + tolerance_rate*V_pie[state_x][state_y])\n",
    "        else:\n",
    "            env_action_right_value = (reward_matrix[i][j] + tolerance_rate*V_pie[i][j])\n",
    "        \n",
    "        value_to_action = desired_action_value*0.8+env_action_left_value*0.1+env_action_right_value*0.1        \n",
    "\n",
    "        value += value_to_action*prob_actions[action]\n",
    "\n",
    "    return value\n",
    "\n",
    "def getPolicy(reward_matrix, cell, V_pie,  tolerance_rate = 1):\n",
    "    \n",
    "    Q_val = [0]*4\n",
    "    actions = ['L', 'R', 'U', 'D']\n",
    "    \n",
    "    \n",
    "    for index, action in enumerate(actions):\n",
    "        state_x, state_y = func(action, cell[0], cell[1])\n",
    "        currQ_Val = 0\n",
    "        if isValid(state_x,state_y):\n",
    "            currQ_Val = (reward_matrix[state_x][state_y] + tolerance_rate*V_pie[state_x][state_y])*0.8\n",
    "        else:\n",
    "            currQ_Val = (reward_matrix[cell[0]][cell[1]] + tolerance_rate*V_pie[cell[0]][cell[1]])*0.8\n",
    "        \n",
    "        state_x,state_y = func(environment_left[action],cell[0],cell[1])\n",
    "        if isValid(state_x,state_y):\n",
    "            currQ_Val = (reward_matrix[state_x][state_y] + tolerance_rate*V_pie[state_x][state_y])*0.1\n",
    "        else:\n",
    "            currQ_Val = (reward_matrix[cell[0]][cell[1]] + tolerance_rate*V_pie[cell[0]][cell[1]])*0.1\n",
    "        \n",
    "        # environment action with 0.1 probability \n",
    "        state_x,state_y = func(environment_right[action],cell[0],cell[1])\n",
    "        if is_valid(state_x,state_y):\n",
    "            currQ_Val = (reward_matrix[state_x][state_y] + tolerance_rate*V_pie[state_x][state_y])*0.1\n",
    "        else:\n",
    "            currQ_Val = (reward_matrix[cell[0]][cell[1]] + tolerance_rate*V_pie[cell[0]][cell[1]])*0.1\n",
    "        \n",
    "        Q_val[index] = currQ_Val\n",
    "        \n",
    "    maxValIndex = np.argmax(Q_val)\n",
    "    return actions[maxValIndex]        \n",
    "    \n",
    "\n",
    "# iterative policy evaluation\n",
    "def iterative_policy_evaluation(iter,theta,reward,reward_matrix,V_pie):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(3):\n",
    "            for j in range(4):\n",
    "                state = (i,j)\n",
    "                if state in terminate_states or state in blocked_State:\n",
    "                    continue\n",
    "                v = V_pie[i][j]\n",
    "                V_pie[i][j] = find_value_function(i,j,reward,reward_matrix)\n",
    "                delta = max(delta,abs(v-V_pie[i][j]))\n",
    "        iter += 1\n",
    "        if delta < theta:\n",
    "            print(f\"Total Iterations :{iter}\")\n",
    "            break \n",
    "            \n",
    "    actionsMatrix = [[\"\" for i in range(4)] for j in range(3)]\n",
    "    for i in range(3):\n",
    "        for j in range(4):\n",
    "            action = getPolicy(reward_matrix, [i, j], V_pie)\n",
    "            actionsMatrix[i][j] = action\n",
    "    print_values(V_pie)\n",
    "\n",
    "def update_reward_matrix(reward):\n",
    "    reward_matrix = [[reward for _ in range(4)] for _ in range(3)]\n",
    "    reward_matrix[2][3] = 1\n",
    "    reward_matrix[1][3] = -1\n",
    "    return reward_matrix\n",
    "    \n",
    "def initialize_V_pie():\n",
    "    V_pie = [[0 for _ in range(4)]for _ in range(3)]\n",
    "    return V_pie    \n",
    "\n",
    "\n",
    "rewards = [-0.04,-2,0.1,0.02,1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for reward in rewards:\n",
    "        print(f\"At r(S) : {reward}\")\n",
    "        reward_matrix = update_reward_matrix(reward)\n",
    "        V_pie = initialize_V_pie()\n",
    "        iterative_policy_evaluation(0,1e-7,reward,reward_matrix,V_pie)\n",
    "        print(\"\\n************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f7465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fc3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7a049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4211374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
